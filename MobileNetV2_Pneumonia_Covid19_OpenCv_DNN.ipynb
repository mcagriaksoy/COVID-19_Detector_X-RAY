{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNetV2_Pneumonia-Covid19_OpenCv_DNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMus8I3uwOYMfRb7jvB59o6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcagriaksoy/Pneumonia_Detector_for_covid19/blob/master/MobileNetV2_Pneumonia_Covid19_OpenCv_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VzrEzGYmJOW",
        "outputId": "e0d40afd-257d-4777-9cd0-1c905b69a094"
      },
      "source": [
        "# -*- coding: utf-8 -*-\r\n",
        "#github.com/mcagriaksoy\r\n",
        "# Mehmet Cagri Aksoy - 2021\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If62dbPo0IZk",
        "outputId": "76d778cc-2a3c-4400-fd58-a0c89875f3f6"
      },
      "source": [
        "try:\r\n",
        "  %tensorflow_version 1.14\r\n",
        "except Exception:\r\n",
        "  pass\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.14`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V-Wq7sGl3-f",
        "outputId": "3f6a26d6-f136-42cc-fbda-e1c8badb0cb5"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "\"\"\"Other Definitions:\"\"\"\r\n",
        "\r\n",
        "# Commented out IPython magic to ensure Python compatibility.\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "# %matplotlib inline\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "sns.set()\r\n",
        "import numpy as np # linear algebra\r\n",
        " # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import  *\r\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\r\n",
        "from tensorflow.keras.applications import DenseNet121, VGG19, ResNet50\r\n",
        "# Input data files are available in the read-only \"../input/\" directory\r\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\r\n",
        "import PIL.Image\r\n",
        "import matplotlib.pyplot as mpimg\r\n",
        "import os\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "\r\n",
        "from tqdm import tqdm\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "# Define path to the data directory\r\n",
        "datasetDir = \"drive/MyDrive/COVID-19/dataset/\"\r\n",
        "\r\n",
        "train_dir = os.path.join(datasetDir, 'train')\r\n",
        "validation_dir = os.path.join(datasetDir, 'validation')\r\n",
        "\r\n",
        "train_normal_dir = os.path.join(train_dir, 'normal')  \r\n",
        "train_covid_dir = os.path.join(train_dir, 'covid')\r\n",
        "\r\n",
        "validation_normal_dir = os.path.join(validation_dir, 'normal') \r\n",
        "validation_covid_dir = os.path.join(validation_dir, 'covid')\r\n",
        "\r\n",
        "batch_size = 8\r\n",
        "epochs = 1\r\n",
        "IMG_HEIGHT = 224\r\n",
        "IMG_WIDTH = 224\r\n",
        "\r\n",
        "image_gen_train = ImageDataGenerator(\r\n",
        "                rotation_range=15,\r\n",
        "                width_shift_range=0.05,\r\n",
        "                height_shift_range=0.05,\r\n",
        "                rescale=1./255,\r\n",
        "                shear_range=0.1,\r\n",
        "                fill_mode='nearest')\r\n",
        "\r\n",
        "image_gen_val = ImageDataGenerator(rescale=1./255)\r\n",
        "\r\n",
        "train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,\r\n",
        "                                                     directory=train_dir,\r\n",
        "                                                     shuffle=True,\r\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\r\n",
        "                                                     class_mode='binary')\r\n",
        "val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\r\n",
        "                                                     directory=validation_dir,\r\n",
        "                                                     shuffle=True,\r\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\r\n",
        "                                                     class_mode='binary')\r\n",
        "\r\n",
        "sample_training_images, _ = next(train_data_gen)\r\n",
        "\r\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape = (224, 224, 3), include_top = False, weights = \"imagenet\")\r\n",
        "\r\n",
        "base_model.trainable = False\r\n",
        "model = tf.keras.Sequential([base_model,\r\n",
        "                                 tf.keras.layers.GlobalAveragePooling2D(),\r\n",
        "                                 tf.keras.layers.Dropout(0.2),\r\n",
        "                                 tf.keras.layers.Dense(1, activation=\"sigmoid\")                                     \r\n",
        "                                ])\r\n",
        "\r\n",
        "base_learning_rate = 0.00001\r\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\r\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "history = model.fit_generator(\r\n",
        "    train_data_gen,\r\n",
        "    steps_per_epoch= train_data_gen.samples // batch_size,\r\n",
        "    epochs=epochs,\r\n",
        "    validation_data=val_data_gen,\r\n",
        "    validation_steps= val_data_gen.samples // batch_size\r\n",
        ")\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 593 images belonging to 2 classes.\n",
            "Found 80 images belonging to 2 classes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "74/74 [==============================] - 81s 1s/step - loss: 0.6624 - acc: 0.5350 - val_loss: 0.7393 - val_acc: 0.4250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXTzzwyimIgt"
      },
      "source": [
        "model.save(\"model.h5\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IenTugVlzlQ6"
      },
      "source": [
        "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\r\n",
        "    \"\"\"\r\n",
        "    Freezes the state of a session into a pruned computation graph.\r\n",
        "\r\n",
        "    Creates a new computation graph where variable nodes are replaced by\r\n",
        "    constants taking their current value in the session. The new graph will be\r\n",
        "    pruned so subgraphs that are not necessary to compute the requested\r\n",
        "    outputs are removed.\r\n",
        "    @param session The TensorFlow session to be frozen.\r\n",
        "    @param keep_var_names A list of variable names that should not be frozen,\r\n",
        "                          or None to freeze all the variables in the graph.\r\n",
        "    @param output_names Names of the relevant graph outputs.\r\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\r\n",
        "    @return The frozen graph definition.\r\n",
        "    \"\"\"\r\n",
        "    graph = session.graph\r\n",
        "    with graph.as_default():\r\n",
        "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\r\n",
        "        output_names = output_names or []\r\n",
        "        output_names += [v.op.name for v in tf.global_variables()]\r\n",
        "        input_graph_def = graph.as_graph_def()\r\n",
        "        if clear_devices:\r\n",
        "            for node in input_graph_def.node:\r\n",
        "                node.device = \"\"\r\n",
        "        frozen_graph = tf.graph_util.convert_variables_to_constants(\r\n",
        "            session, input_graph_def, output_names, freeze_var_names)\r\n",
        "        return frozen_graph"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "Dscs2GMszmbY",
        "outputId": "0c73142d-9181-43b1-9b17-ff816bbcaabb"
      },
      "source": [
        "from keras import backend as K\r\n",
        "\r\n",
        "# Create, compile and train model...\r\n",
        "\r\n",
        "frozen_graph = freeze_session(K.get_session(),\r\n",
        "                              output_names=[out.op.name for out in model.outputs])\r\n",
        "\r\n",
        "tf.train.write_graph(frozen_graph, \"some_directory\", \"my_model.pb\", as_text=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-0d92ab84a915>:26: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Froze 271 variables.\n",
            "INFO:tensorflow:Converted 271 variables to const ops.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'some_directory/my_model.pb'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "_sb_EQiEmET0",
        "outputId": "80199c67-a89b-4e2a-c3c7-fd83eb123569"
      },
      "source": [
        "import cv2\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "# Load a model imported from Tensorflow\r\n",
        "tensorflowNet = cv2.dnn.readNetFromTensorflow('/content/some_directory/my_model.pb')\r\n",
        "\r\n",
        "# Input image\r\n",
        "img = cv2.imread('/content/1.jpeg')\r\n",
        "rows, cols, channels = img.shape\r\n",
        "\r\n",
        "# Use the given image as input, which needs to be blob(s).\r\n",
        "tensorflowNet.setInput(cv2.dnn.blobFromImage(img, size=(300, 300), swapRB=True, crop=False))\r\n",
        "\r\n",
        "# Runs a forward pass to compute the net output\r\n",
        "networkOutput = tensorflowNet.forward()\r\n",
        "\r\n",
        "# Loop on the outputs\r\n",
        "for detection in networkOutput[0,0]:\r\n",
        "\r\n",
        "    score = float(detection[2])\r\n",
        "    if score > 0.9:\r\n",
        "\r\n",
        "        left = detection[3] * cols\r\n",
        "        top = detection[4] * rows\r\n",
        "        right = detection[5] * cols\r\n",
        "        bottom = detection[6] * rows\r\n",
        "\r\n",
        "        #draw a red rectangle around detected objects\r\n",
        "        cv2.rectangle(img, (int(left), int(top)), (int(right), int(bottom)), (0, 0, 255), thickness=2)\r\n",
        "\r\n",
        "# Show the image with a rectagle surrounding the detected objects \r\n",
        "cv2_imshow(img)\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-03927efd8f85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Runs a forward pass to compute the net output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mnetworkOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorflowNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Loop on the outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/dnn/src/dnn.cpp:525: error: (-2:Unspecified error) Can't create layer \"mobilenetv2_1.00_224/bn_Conv1/cond/FusedBatchNormV3_1\" of type \"FusedBatchNormV3\" in function 'getLayerInstance'\n"
          ]
        }
      ]
    }
  ]
}